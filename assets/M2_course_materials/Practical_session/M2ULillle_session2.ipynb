{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 2: Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Let's start by loading all the required libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from typing import Sequence, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(64) # Fixing the seed for reproducibility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of clinical trials, Phase I trials are the first stage of testing in human subjects. Their goal is to evaluate the safety (and feasibility) of the treatment and identify its side effects. The aim of a phase I dose-finding study is to determine the most appropriate dose level that should be used in further phases of the clinical trials. Traditionally, the focus is on determining the highest dose with acceptable toxicity called the Maximum Tolerated Dose (MTD).\n",
    "\n",
    "A dose-finding study involves a number K of dose levels that have been chosen by physicians based on preliminary experiments (K is usually a number between 3 and 10). Denoting by $p_k$ the (unknown) toxicity probability of dose $k$, the Maximum Tolerated Dose (MTD) is defined as the dose with a toxicity probability closest to a target:\n",
    "\n",
    "\\begin{align}\n",
    "k^* \\in \\underset{k\\in\\{1,\\dots,K\\}}{\\mathrm{argmin}}|\\theta - p_k|\n",
    "\\end{align}\n",
    "\n",
    "where $\\theta$ is the pre-specified targeted toxicity probability (typically between 0.2 and 0.35).\n",
    "A MTD identification algorithm proceeds sequentially: at round $t$ a dose $D_t \\in \\{1, \\dots , K\\}$ is selected and administered to a patient for whom a toxicity response is observed. A binary outcome $X_t$ is revealed where $X_t = 1$ indicates that a harmful side-effect occurred and $X_t = 0$ indicates that no harmful side-effect occurred. We assume that $X_t$ is drawn from a Bernoulli distribution with mean $p_{D_t}$ and is independent from previous observations.\n",
    "\n",
    "**Hint**: In this example, the reward definition is a bit different from the usual case. We would like to take the arm with minimum $|\\theta - \\hat{p}_k|$ where $\\hat{p}_k$ is the estimated toxicity probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MultiArmedBandit environment\n",
    "\n",
    "The MultiArmedBandit object takes as parameters:\n",
    "\n",
    "1. the number of arms\n",
    "2. the function from which to sample the mean reward of each arm\n",
    "3. the function to sample the observed rewards for each arm given its mean reward\n",
    "\n",
    "## Task 1: Define your Bandit class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the class has been written. Complete the pull method in such a way that:\n",
    "\n",
    "**1.** Update both `num_dose_selected` and `num_toxic` arrays,\n",
    "\n",
    "**2.** Compute and return the reward $-|\\theta - \\hat{p}_k|$ where $\\hat{p}_k$ is the estimated toxicity probability of arm $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               n_arm: int = 2,\n",
    "               n_pulls: int = 2000,\n",
    "               actual_toxicity_prob: list = [0.4, 0.6],\n",
    "               theta: float = 0.3,\n",
    "               ):\n",
    "    self.n_arm = n_arm\n",
    "    self.n_pulls = n_pulls\n",
    "    self.actual_toxicity_prob = actual_toxicity_prob\n",
    "    self.theta = theta\n",
    "    self.init_bandit()\n",
    "\n",
    "  def init_bandit(self):\n",
    "    \"\"\"\n",
    "      Pessimistic initializaiton\n",
    "    \"\"\"\n",
    "    self.num_dose_selected = np.array([0]*self.n_arm) # number of times a dose is selected\n",
    "    self.num_toxic = np.array([0]*self.n_arm) # number of times a does found to be toxic\n",
    "\n",
    "  def pull(self, a_idx: int):\n",
    "    \"\"\"\n",
    "    .inputs:\n",
    "      a_idx: Index of action.\n",
    "    .outputs:\n",
    "      rew: reward value.\n",
    "    \"\"\"\n",
    "    assert a_idx < self.n_arm, \"invalid action index\"\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "    return rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the setting for the rest of the practical session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a dose finding study with three doses ($K = 3$) where you need to choose from with `actual_toxicity_prob=[0.1, 0.35, 0.8]` and targeted toxicity probability is $\\theta = 0.3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Problem definition\n",
    "bandit = Bandit(n_arm=3, n_pulls=2000, actual_toxicity_prob=[0.1, 0.35, 0.8], theta=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.b: $\\epsilon$-greedy for k-armed bandit and Optimistic initial values \n",
    "\n",
    "#### Q1.b1: $\\epsilon$-greedy algorithm implementation \n",
    "\n",
    "Implement the $\\epsilon$-greedy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(\n",
    "    bandit: Bandit,\n",
    "    eps: float,\n",
    "    init_q: float = 0.0\n",
    ") -> Tuple[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    .inputs:\n",
    "      bandit: A bandit problem, instantiated from the above class.\n",
    "      eps: The epsilon value for exploration.\n",
    "      init_q: Initial estimation of each arm's value.\n",
    "    .outputs:\n",
    "      rew_record: The record of rewards at each timestep.\n",
    "      avg_ret_record: The average of rewards up to step t, where t goes from 0 to n_pulls.\n",
    "      tot_reg_record: The regret up to step t, where t goes from 0 to n_pulls.\n",
    "      opt_action_perc_record: Percentage of optimal arm selected.\n",
    "    \"\"\"\n",
    "    # Initialize q values and counts\n",
    "    q = np.array([init_q] * bandit.n_arm, dtype=float)  # Action value estimates\n",
    "    n_a = np.zeros(bandit.n_arm)  # Number of times each arm is selected\n",
    "\n",
    "    cumulative_reward = 0.0\n",
    "    cumulative_regret = 0.0\n",
    "    rew_record = []\n",
    "    avg_ret_record = []\n",
    "    tot_reg_record = []\n",
    "    opt_action_perc_record = []\n",
    "\n",
    "  for t in range(bandit.n_pulls):\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "\n",
    "  return rew_record, avg_ret_record, tot_reg_record, opt_action_perc_record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.b2: Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the driver code provided to plot: \n",
    "\n",
    "(1) The average return\n",
    "\n",
    "(2) The reward\n",
    "\n",
    "(3) the total regret\n",
    "\n",
    " (4) the percentage of optimal action across the $N$=20 runs as a function of the number of pulls (2000 pulls for each run) for all three $\\epsilon$ values of 0.5, 0.1, and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "plt.figure(1)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")\n",
    "plt.figure(3)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"% optimal action\")\n",
    "\n",
    "N = 20\n",
    "tot_reg_rec_best = 1e8\n",
    "\n",
    "for eps in [0.5, 0.1, .0]:\n",
    "  rew_rec = np.zeros(bandit.n_pulls)\n",
    "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
    "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
    "  opt_act_rec = np.zeros(bandit.n_pulls)\n",
    "  start_time = time.time()\n",
    "  for n in range(N):\n",
    "    bandit.init_bandit()\n",
    "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = eps_greedy(bandit, eps)\n",
    "    rew_rec += np.array(rew_rec_n)\n",
    "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
    "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
    "    opt_act_rec += np.array(opt_act_rec_n)\n",
    "\n",
    "  end_time = time.time()\n",
    "  # print(f\"time per run: {end_time - start_time}/N\")\n",
    "  # take the mean\n",
    "  rew_rec /= N\n",
    "  avg_ret_rec /= N\n",
    "  tot_reg_rec /= N\n",
    "  opt_act_rec /= N\n",
    "\n",
    "  plt.figure(0)\n",
    "  plt.plot(avg_ret_rec, label=\"eps={}\".format(eps))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(1)\n",
    "  plt.plot(rew_rec[1:], label=\"eps={}\".format(eps))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(2)\n",
    "  plt.plot(tot_reg_rec, label=\"eps={}\".format(eps))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(3)\n",
    "  plt.plot(opt_act_rec, label=\"eps={}\".format(eps))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
    "        ep_greedy_dict = {\n",
    "        'opt_act':opt_act_rec,\n",
    "        'regret_list':tot_reg_rec,}\n",
    "        tot_reg_rec_best = tot_reg_rec[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "Explain the results from the perspective of exploration and how different $\\epsilon$ values affect the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.b4: Optimistic Initial Values \n",
    "\n",
    "We want to run the optimistic initial value method on the same problem described above for the initial q values of -1 and +1 for all arms. Compare its performance, measured by the average reward across $N$=20 runs as a function of the number of pulls, with the non-optimistic setting with initial q values of 0 for all arms. For both optimistic and non-optimistic settings, $\\epsilon$=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "\n",
    "plt.figure(5)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"reward\")\n",
    "\n",
    "plt.figure(6)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")\n",
    "\n",
    "N = 20\n",
    "for init_q in [-1, 1]:\n",
    "  rew_rec = np.zeros(bandit.n_pulls)\n",
    "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
    "  for n in range(N):\n",
    "    bandit.init_bandit()\n",
    "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = eps_greedy(bandit, eps=0.0, init_q=init_q)\n",
    "\n",
    "    rew_rec += np.array(rew_rec_n)\n",
    "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
    "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
    "\n",
    "  avg_ret_rec /= N\n",
    "  rew_rec /= N\n",
    "  tot_reg_rec /= N\n",
    "  plt.figure(4)\n",
    "  plt.plot(avg_ret_rec[1:], label=\"q_init_val={}\".format(init_q))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(5)\n",
    "  plt.plot(rew_rec[1:], label=\"q_init_val={}\".format(init_q))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(6)\n",
    "  plt.plot(tot_reg_rec[1:], label=\"q_init_val={}\".format(init_q))\n",
    "  plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "Explain how initial q values affect the exploration and the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper confidence bound arm selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c: Upper-Confidence-Bound action selection (15 points)\n",
    "\n",
    "#### Q1.c1: UCB algorithm implementation (5 points)\n",
    "Implement the UCB algorithm on the same MAB problem as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(\n",
    "    bandit: Bandit,\n",
    "    c: float,\n",
    "    init_q: float = 0.0\n",
    ") -> Tuple[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    .inputs:\n",
    "      bandit: A bandit problem, instantiated from the above class.\n",
    "      c: The exploration coefficient.\n",
    "      init_q: Initial estimation of each arm's value.\n",
    "    .outputs:\n",
    "      rew_record: The record of rewards at each timestep.\n",
    "      avg_ret_record: The average summation of rewards up to step t, where t goes from 0 to n_pulls.\n",
    "      tot_reg_record: The regret up to step t, where t goes from 0 to n_pulls.\n",
    "      opt_action_perc_record: Percentage of optimal arm selected.\n",
    "    \"\"\"\n",
    "    # Initialize action values, counts, and metrics\n",
    "    q = np.array([init_q] * bandit.n_arm, dtype=float)  # Estimated values of each arm\n",
    "    n_a = np.zeros(bandit.n_arm)  # Number of times each arm has been pulled\n",
    "    ret = 0.0  # Cumulative reward\n",
    "    rew_record = []  # Reward at each time step\n",
    "    avg_ret_record = []  # Average return at each time step\n",
    "    tot_reg_record = []  # Cumulative regret at each time step\n",
    "    opt_action_perc_record = []  # Percentage of optimal actions\n",
    "\n",
    "    # Determine the optimal arm\n",
    "    actual_toxicity_prob = np.array(bandit.actual_toxicity_prob)\n",
    "    opt_arm = np.argmin(np.abs(actual_toxicity_prob - bandit.theta))\n",
    "\n",
    "    optimal_action_count = 0  # Counter for the number of times the optimal arm is selected\n",
    "    cumulative_regret = 0.0  # Cumulative regret\n",
    "\n",
    "  for t in range(bandit.n_pulls):\n",
    "    # Assuming to take the first arm always when there is no exploration\n",
    "    # ----------------------------------------------\n",
    "    ...\n",
    "    # ----------------------------------------------\n",
    "\n",
    "\n",
    "  return rew_record, avg_ret_record, tot_reg_record, opt_action_perc_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.c2: Plotting the results\n",
    "\n",
    "Use the driver code provided to plot: (1) The average return, (2) The reward, (3) the total regret, and (4) the percentage of optimal action across the $N$=20 runs as a function of the number of pulls (2000 pulls for each run) for three values of $c$=0, 0.5, and 2.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(7)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"avg return\")\n",
    "plt.figure(8)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.figure(9)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"total regret\")\n",
    "plt.figure(10)\n",
    "plt.xlabel(\"n pulls\")\n",
    "plt.ylabel(\"% optimal action\")\n",
    "\n",
    "N = 20\n",
    "tot_reg_rec_best = 1e8\n",
    "for c in [.0, 0.5, 2]:\n",
    "  rew_rec = np.zeros(bandit.n_pulls)\n",
    "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
    "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
    "  opt_act_rec = np.zeros(bandit.n_pulls)\n",
    "\n",
    "  for n in range(N):\n",
    "    bandit.init_bandit()\n",
    "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = ucb(bandit, c)\n",
    "    rew_rec += np.array(rew_rec_n)\n",
    "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
    "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
    "    opt_act_rec += np.array(opt_act_rec_n)\n",
    "\n",
    "  # take the mean\n",
    "  rew_rec /= N\n",
    "  avg_ret_rec /= N\n",
    "  tot_reg_rec /= N\n",
    "  opt_act_rec /= N\n",
    "\n",
    "  plt.figure(7)\n",
    "  plt.plot(avg_ret_rec, label=\"c={}\".format(c))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(8)\n",
    "  plt.plot(rew_rec, label=\"c={}\".format(c))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(9)\n",
    "  plt.plot(tot_reg_rec, label=\"c={}\".format(c))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  plt.figure(10)\n",
    "  plt.plot(opt_act_rec, label=\"c={}\".format(c))\n",
    "  plt.legend(loc=\"lower right\")\n",
    "\n",
    "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
    "        ucb_dict = {\n",
    "        'opt_act':opt_act_rec,\n",
    "        'regret_list':tot_reg_rec,}\n",
    "        tot_reg_rec_best = tot_reg_rec[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.c3: Analysis \n",
    "\n",
    "\n",
    " Explain the results from the perspective of exploration and how different $c$ values affect the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
